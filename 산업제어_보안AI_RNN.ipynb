{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "산업제어 보안AI_RNN",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1BU2SSYNPX9KtyvlwChAcxYhvJ9ZlJZI7",
      "authorship_tag": "ABX9TyP9vOm3Tce9B6pSkyWaSWgV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gihoonpark/Security-Threat-Detection-Anomaly-Detection-Time-series/blob/main/%EC%82%B0%EC%97%85%EC%A0%9C%EC%96%B4_%EB%B3%B4%EC%95%88AI_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdfVjE_moLpS"
      },
      "source": [
        "Library\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qw0IxPEoK5u"
      },
      "source": [
        "import sys\r\n",
        "\r\n",
        "from pathlib import Path\r\n",
        "from datetime import timedelta\r\n",
        "import dateutil\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from tqdm.notebook import trange\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikc4Mbi8oRvA"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jqAADsbQqZL"
      },
      "source": [
        "train1 = pd.read_csv('/content/drive/My Drive/dataset/산업보안AI_data/HAI 2.0/training/train1.csv')\n",
        "train2 = pd.read_csv('/content/drive/My Drive/dataset/산업보안AI_data/HAI 2.0/training/train2.csv')\n",
        "train3 = pd.read_csv('/content/drive/My Drive/dataset/산업보안AI_data/HAI 2.0/training/train3.csv')\n",
        "\n",
        "TRAIN_DF_RAW = pd.concat([train1, train2, train3], axis=0)\n",
        "TRAIN_DF_RAW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs2rZd-6Rq0u"
      },
      "source": [
        "TIMESTAMP_FIELD = \"time\"\n",
        "IDSTAMP_FIELD = 'id'\n",
        "ATTACK_FIELD = \"attack\"\n",
        "VALID_COLUMNS_IN_TRAIN_DATASET = TRAIN_DF_RAW.columns.drop([TIMESTAMP_FIELD])\n",
        "VALID_COLUMNS_IN_TRAIN_DATASET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QooTodA2VJtp"
      },
      "source": [
        "sns.heatmap(TRAIN_DF_RAW.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5VbHH6lofOD"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SED0lKfwS1Uh"
      },
      "source": [
        "# train_dataset field별 min, maxr값\n",
        "TAG_MIN = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
        "TAG_MAX = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].max() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S6x-0tkS2Ts"
      },
      "source": [
        "def normalize(df):\n",
        "    ndf = df.copy()\n",
        "    for c in df.columns:\n",
        "        if TAG_MIN[c] == TAG_MAX[c]: # 최소, 최대값이 같은 필드의 값들은 모두 0으로 만듦\n",
        "            ndf[c] = df[c] - TAG_MIN[c]\n",
        "        else:\n",
        "            ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c]) # 위 경우 외 정규화\n",
        "    return ndf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5393vZKTan4"
      },
      "source": [
        "TRAIN_DF = normalize(TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET])\n",
        "np.array(TRAIN_DF) #(921603, 80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZYMkqYxTprz"
      },
      "source": [
        "'''\n",
        "TIME_STEPS = 90\n",
        "\n",
        "def create_sequences(values, time_steps=TIME_STEPS):\n",
        "    output = []\n",
        "    for i in range(len(values) - time_steps):\n",
        "        output.append(values[i : (i + time_steps)])\n",
        "    # Convert 2D sequences into 3D as we will be feeding this into\n",
        "    # a convolutional layer.\n",
        "    return np.expand_dims(output, axis=2) # cnn model로 들어갈 input이기 때문에 차원 추가.\n",
        "\n",
        "x_train = create_sequences(TRAIN_DF)\n",
        "print(\"Training input shape: \", x_train.shape)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFLcA1bmWSRN"
      },
      "source": [
        "# 시계열 모형 input에 적합하도록 전처리(window, pad sequence)\n",
        "\n",
        "WINDOW_GIVEN = 89\n",
        "WINDOW_SIZE = 90\n",
        "\n",
        "class HaiDataset():\n",
        "    def __init__(self, timestamps, df, stride=10, attacks=None):\n",
        "        self.ts = np.array(timestamps)\n",
        "        self.tag_values = np.array(df, dtype=np.float32)\n",
        "        self.valid_idxs = []\n",
        "        for L in trange(len(self.ts) - WINDOW_SIZE + 1): # trange(921603-90+1 = 921514) , L = 0,1,2,...,921513\n",
        "            R = L + WINDOW_SIZE - 1 # R = 89,90,91,...,921602\n",
        "            if dateutil.parser.parse(self.ts[R]) - dateutil.parser.parse( \n",
        "                self.ts[L] # parse('2016-04-16')-> datetime.datetime(2016, 4, 16, 0, 0) , 90번째 timestap 마다 같은지 확인\n",
        "                # dateutil.parser.parse(self.ts[R]): 89,90,..921602 번째 timestap을 datetime형 데이터로 바꾸고,\n",
        "                # dateutil.parser.parse(self.ts[L]): 0,1,2,...921513번째 timestap을 datetime형 데이터로 바꿔서 이 둘 차이가 만약 timedelta(89초)이면 if문 실행\n",
        "            ) == timedelta(seconds=WINDOW_SIZE - 1): # timedelta : 두 날짜/시간의 차이인 기간.\n",
        "                self.valid_idxs.append(L) # 90번째 timstap가 될때마다 timestap의 인덱스 L을 valid_idxs에 추가. valid_idxs = (L = 0,1,2,...921513 )\n",
        "        self.valid_idxs = np.array(self.valid_idxs, dtype=np.int32)[::stride] # valid_idxs=(0,1,2,...921513)를 10번째씩 추출, valid_idxs 재구성, 92134개 생김\n",
        "        self.n_idxs = len(self.valid_idxs) # n_idxs = 92134 , valid_idxs=[0  10  20 ... 921488 921498 921508] -> 아마 데이터가 항상 1초 간격은 아님을 확인\n",
        "        print(f\"# of valid windows: {self.n_idxs}\") # n_idxs = 92134\n",
        "        if attacks is not None: # 현재는 attacks = None이므로 HaiDataset class의 객체인 HAI_DATASET_TRAIN.with_attacks는 False이다.\n",
        "            self.attacks = np.array(attacks, dtype=np.float32)\n",
        "            self.with_attack = True\n",
        "        else:\n",
        "            self.with_attack = False\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_idxs\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i = self.valid_idxs[idx] #  idx로 인덱싱 가능 ///  idx=0 이면, i=0. idx=1 이면, i=10. ...  (valid_idxs=(0,1,2,...921513))\n",
        "        last = i + WINDOW_SIZE - 1 # last = [89, 99, 109, ...]\n",
        "        item = {\"attack\": self.attacks[last]} if self.with_attack else {}\n",
        "        item[\"ts\"] = self.ts[i + WINDOW_SIZE - 1] # timestap ts : [89, 99, 109, ...] 데이터들 , 2020-09-11-00-01-29 , 2020-09-11-00-01-39, ...\n",
        "        item[\"given\"] = torch.from_numpy(self.tag_values[i : i + WINDOW_GIVEN]) \n",
        "        # torch.from_numpy :numpy를 tensor 자료형으로 바꿈, df데이터의 0~88, 10~98, 20~108, ...,921508~921596 index를 가지는 input 데이터 (시계열데이터수92134,timestap89,다변량80) \n",
        "        item[\"answer\"] = torch.from_numpy(self.tag_values[last]) # 0~88, 10~98, 20~108... given 시계열 데이터로 89, 99, 109, ...시계열데이터 예측하기전, 정답 output 데이터.\n",
        "        return item\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID9oS7eHWYPO"
      },
      "source": [
        "HAI_DATASET_TRAIN = HaiDataset(TRAIN_DF_RAW[TIMESTAMP_FIELD], TRAIN_DF, stride=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlxPq4JIpuP7"
      },
      "source": [
        "Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7WTt9WfY6K4"
      },
      "source": [
        "N_HIDDENS = 100\n",
        "N_LAYERS = 3\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "class StackedGRU(torch.nn.Module):\n",
        "    def __init__(self, n_tags):  # layer 정의.\n",
        "        super().__init__() \n",
        "        # stackedGRU 클래스는 torch.nn.Module 클래스를 상속받음\n",
        "        # 먼저 stackedGRU의 n_tag 속성을 부르고, super().__init__통해 torch.nn.Module 의 __init__ 호출.\n",
        "        self.rnn = torch.nn.GRU(\n",
        "            input_size=n_tags, # n_tags = 80\n",
        "            hidden_size=N_HIDDENS,\n",
        "            num_layers=N_LAYERS,\n",
        "            bidirectional=True,\n",
        "            dropout=0, # batch_first를 True로 안하면 time-step(=sequence_length), batch_size, input_vector 형태.\n",
        "        )\n",
        "        self.fc = torch.nn.Linear(N_HIDDENS * 2, n_tags) # N_HIDDENS * 2의 2는 bidirectional_lstm때문, # output_tensor : (Batch, seq=89, n_tags = 80)\n",
        "\n",
        "    def forward(self, x):  # 위에서 정의한 layer로 데이터 x 정의, foward 훈련 과정 정의.\n",
        "        x = x.transpose(0, 1)  # 기존 x : (batch, seq, params) -> 변화 x : (seq, batch, params), 왜할까? -> 위에서 batch_first = True로 안했기 때문.\n",
        "        self.rnn.flatten_parameters() # cell간 compact해야해서 호출? -? 개인적으로 공부 필요\n",
        "        # RNN 셀은 두 개의 입력을 리턴, 첫번째 리턴값은 모든 시점(timesteps)의 은닉 상태, output : seq_output\n",
        "        # 두번째 리턴값은 마지막 시점(timestep)의 은닉 상태, hidden_state_output : \n",
        "        # 요약 : (output, (hidden or hidden,cell)) 의 tuple 형태, LSTM만 cell state있음\n",
        "        outs, _ = self.rnn(x)\n",
        "        out = self.fc(outs[-1]) # many to one이므로 seq_output 중 가장 마지막 seq이 필요. \n",
        "        return x[0] + out # 왜 x[0]??? -> 각 배치 데이터의 첫번째 seq (batch_size, n_tage) 데이터들을 rnn output값에 더해서 skip-connection 구현.\n",
        "        # 단순히 rnn ouptut만 도출해도 무방함."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr703LlGZpaj"
      },
      "source": [
        "MODEL = StackedGRU(n_tags=TRAIN_DF.shape[1]) # TRAIN_DF.shape[1] = 80\n",
        "MODEL.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ0Qyxmopz8W"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5O1VB3WZpZq"
      },
      "source": [
        "def train(dataset, model, batch_size, n_epochs):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) #HAI_DATASET_TRAIN호출, 배치셋 만듦\n",
        "    optimizer = torch.optim.AdamW(model.parameters()) # optimizer는 adamW로 설정.\n",
        "    loss_fn = torch.nn.MSELoss() # 손실함수는 MSE로 설정.\n",
        "    epochs = trange(n_epochs, desc=\"training\") # epochs = [0,1,2,3,4,... n_epochs]\n",
        "    best = {\"loss\": sys.float_info.max}\n",
        "    loss_history = []\n",
        "    for e in epochs:\n",
        "        epoch_loss = 0\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad() # 초기화, backward()를 호출할 때마다 변화도가 버퍼(buffer)에 누적되기 때문.\n",
        "            given = batch[\"given\"].cuda() # 배치셋의 given 데이터를 x = given으로 정의 \n",
        "            guess = model(given) # MODEL에 x 대입 -> (x:given) -> y_hat 도출 (y_hat = guess)\n",
        "            answer = batch[\"answer\"].cuda() # 배치셋의 answer 데이터를 y = given으로 정의 \n",
        "            loss = loss_fn(answer, guess) # answer과 guess 간 손실함수 대입해서 loss 값 구함\n",
        "            loss.backward() # 역전파 단계: 모델의 매개변수에 대한 손실의 변화도를 계산.\n",
        "            epoch_loss += loss.item() # loss는 (1,) 형태의 Tensor이며, loss.item()은 loss의 스칼라 값\n",
        "            optimizer.step() # Optimizer의 step 함수를 호출하면 매개변수가 갱신\n",
        "\n",
        "        loss_history.append(epoch_loss) # loss_history : 배치별 loss 값의 합을 loss_history에 저장.\n",
        "        epochs.set_postfix_str(f\"loss: {epoch_loss:.6f}\")\n",
        "        if epoch_loss < best[\"loss\"]: #  가장 작은 loss를 찾을때까지 계속 best 사전의 loss item값을 epcoh마다 업데이트\n",
        "            best[\"state\"] = model.state_dict() \n",
        "            # torch.nn.Module 모델의 학습 가능한 매개변수(예. 가중치와 편향)들은 모델의 매개변수에 포함되어 있다 (model.parameters()로 접근)\n",
        "            # state_dict 는 간단히 말하면 각 계층을 매개변수 텐서로 매핑되는 Python 사전(dict) 객체\n",
        "            best[\"loss\"] = epoch_loss\n",
        "            best[\"epoch\"] = e + 1\n",
        "    return best, loss_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTRKW1EWZzs8"
      },
      "source": [
        "%%time\n",
        "MODEL.train()\n",
        "BEST_MODEL, LOSS_HISTORY = train(HAI_DATASET_TRAIN, MODEL, BATCH_SIZE, 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIAZWXbfZ3Rl"
      },
      "source": [
        "BEST_MODEL[\"loss\"], BEST_MODEL[\"epoch\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M3zs3m4FDvk"
      },
      "source": [
        "import os\r\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lkzyb-Ep2ln"
      },
      "source": [
        "Save and load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsi5oNJUFK05"
      },
      "source": [
        " torch.save({\r\n",
        "     \"state\": BEST_MODEL[\"state\"],\r\n",
        "     \"best_epoch\": BEST_MODEL[\"epoch\"],\r\n",
        "     \"loss_history\": LOSS_HISTORY,\r\n",
        "     }, '/content/drive/MyDrive/dataset/산업보안AI_data/' + 'model.pt')\r\n",
        "# 전체 모델을 저장하거나, 모델의 state_dict를 저장 할 때 사용."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0VRbd4L_GFr"
      },
      "source": [
        "SAVED_MODEL = torch.load('/content/drive/MyDrive/dataset/산업보안AI_data/' + 'model.pt') \n",
        "# 전체 모델을 불러오거나, 모델의 state_dict를 불러 올 때 사용\n",
        "\n",
        "MODEL.load_state_dict(SAVED_MODEL[\"state\"]) \n",
        "# state_dict를 이용하여, 모델 객체 내의 매개 변수 값을 초기화.\n",
        "# 모델을 불러 온 이후에는 이 모델을 학습 할 껀지, 사용 할 껀지에 따라 각각 model.train(), model.eval() 둘 중에 하나를 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqcrCQraqDYA"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ML6PQpx5aII"
      },
      "source": [
        "test1 = pd.read_csv('/content/drive/My Drive/dataset/산업보안AI_data/HAI 2.0/testing/test1.csv')\n",
        "test2 = pd.read_csv('/content/drive/My Drive/dataset/산업보안AI_data/HAI 2.0/testing/test2.csv')\n",
        "test3= pd.read_csv('/content/drive/My Drive/dataset/산업보안AI_data/HAI 2.0/testing/test3.csv')\n",
        "test4= pd.read_csv('/content/drive/My Drive/dataset/산업보안AI_data/HAI 2.0/testing/test4.csv')\n",
        "TEST_DF_RAW = pd.concat([test1, test2, test3, test4], axis=0)\n",
        "TEST_DF_RAW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYhWSbNw51W3"
      },
      "source": [
        "TEST_DF = normalize(TEST_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
        "TEST_DF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FXZreZe55R5"
      },
      "source": [
        "HAI_DATASET_TEST = HaiDataset(TEST_DF_RAW[TIMESTAMP_FIELD], TEST_DF, stride=1, attacks=None) # stride=1로 모든 데이터를 인코딩 할 수 있게함."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hx6eAf-y-RzB"
      },
      "source": [
        "HAI_DATASET_TEST[0], HAI_DATASET_TEST[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mazv7bcD55RJ"
      },
      "source": [
        "def inference(dataset, model, batch_size):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    ts, dist, att = [], [], []\n",
        "    with torch.no_grad(): # 해당 블록을 history 트래킹 하지 않겠다는 뜻. / 학습이 아닌 추론이므로 loss, optimzer grad, step 등등 없음.\n",
        "        for batch in dataloader: # dataloader -> HAI_DATASET_TEST를 배치셋화 함. (batch_size, seq=1, n_tag = 80)\n",
        "            given = batch[\"given\"].cuda()\n",
        "            answer = batch[\"answer\"].cuda()\n",
        "            guess = model(given) # given x값을 학습된 model에 넣어 y_hat 도출. \n",
        "            ts.append(np.array(batch[\"ts\"]))\n",
        "            dist.append(torch.abs(answer - guess).cpu().numpy()) # y와 y_hat 값의 차이\n",
        "            try: # 실행할 코드, 현재는 batch에 'attack' 속성이 없음.. - 아마도??\n",
        "                att.append(np.array(batch[\"attack\"]))\n",
        "            except: # 실행할 코드가 안될시, att에 batchsize 만큼의 0 넣기 (최종적으로 test data 수 만큼 0이 넣어짐.)\n",
        "                att.append(np.zeros(batch_size))\n",
        "            \n",
        "    return (\n",
        "        np.concatenate(ts), # CHECK_TS\n",
        "        np.concatenate(dist), # CHECK_DIST\n",
        "        np.concatenate(att), # CHECK_ATT\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqpTgqCw6KRt"
      },
      "source": [
        "def put_labels(distance, threshold):\n",
        "    xs = np.zeros_like(distance)# ANOMALY_SCORE = distance 행렬과 같은 shape의 0으로 이루어진 numpy 행렬 xs 만듦. (358448,)\n",
        "    xs[distance > threshold] = 1 # distance행렬 각 원소 > threshold 이면, xs numy 행렬의 그 위치에 1.\n",
        "    return xs (358448,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFSILR726Y-4"
      },
      "source": [
        "%%time\n",
        "# 모델을 불러 온 이후에는 이 모델을 학습 할 껀지, 사용 할 껀지에 따라 각각 model.train(), model.eval() 둘 중에 하나를 사용.\n",
        "MODEL.eval() \n",
        "CHECK_TS, CHECK_DIST, CHECK_ATT = inference(HAI_DATASET_TEST, MODEL, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGnlJpEPMC9n"
      },
      "source": [
        "CHECK_TS, CHECK_DIST, CHECK_ATT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMetjSf66Zdi"
      },
      "source": [
        "ANOMALY_SCORE = np.mean(CHECK_DIST, axis=1) \r\n",
        "# test데이터에서 inference함수로 도출된 y_hat값과 y의 차이로 구해진 1seq 내 모든 n_tag(80개)의 평균 (열 평균이므로 axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74ozN8icNhnL"
      },
      "source": [
        "ANOMALY_SCORE.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbn56tqH6ZkO"
      },
      "source": [
        "def check_graph(xs, att, piece=2, THRESHOLD=None):\n",
        "    l = xs.shape[0] # 전체 ANOMALY_SCORE 데이터 수 = 358448\n",
        "    chunk = l // piece # 전체 데이터 수 358448을 piece = 3로 나눈 값, 이 값이 그래프 위에서 아래로 3개 중 각각 xtick 범위가 됨. chunk=119482.xx\n",
        "    fig, axs = plt.subplots(piece, figsize=(20, 4 * piece))\n",
        "    for i in range(piece): # i=0,1,2 \n",
        "        L = i * chunk # L = 0, 119482, 23xxxx\n",
        "        R = min(L + chunk, l) # R = min(119482,358448), min(23xxxx,358448), min(36xxxx,358448)\n",
        "        xticks = range(L, R) # R그래프 3개 X범위 -> L,R = 0 ~ 12000, 12000 ~ 24000, 24000 ~ 36000\n",
        "        axs[i].plot(xticks, xs[L:R])\n",
        "        if len(xs[L:R]) > 0:\n",
        "            peak = max(xs[L:R])\n",
        "            axs[i].plot(xticks, att[L:R] * peak * 0.3)\n",
        "        if THRESHOLD!=None:\n",
        "            axs[i].axhline(y=THRESHOLD, color='r') # THRESHOLD가 NONE이 아니면, 빨간색으로 y = THRESHOLD 선을 긋는다.\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXxUgcre6Zjf"
      },
      "source": [
        "THRESHOLD = 0.045\n",
        "check_graph(ANOMALY_SCORE, CHECK_ATT, piece=3, THRESHOLD=THRESHOLD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8frXz0bO6fiu"
      },
      "source": [
        "LABELS = put_labels(ANOMALY_SCORE, THRESHOLD) #(358448,)\n",
        "LABELS, LABELS.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQiFh1OKRFkx"
      },
      "source": [
        "LABELS.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZa5c2MwLfKv"
      },
      "source": [
        "def fill_blank(check_ts, labels, total_ts):\r\n",
        "    def ts_generator():\r\n",
        "        for t in total_ts:\r\n",
        "            yield dateutil.parser.parse(t)\r\n",
        "\r\n",
        "    def label_generator():\r\n",
        "        for t, label in zip(check_ts, labels):\r\n",
        "            yield dateutil.parser.parse(t), label\r\n",
        "\r\n",
        "    g_ts = ts_generator()\r\n",
        "    g_label = label_generator()\r\n",
        "    final_labels = []\r\n",
        "\r\n",
        "    try:\r\n",
        "        current = next(g_ts)\r\n",
        "        ts_label, label = next(g_label)\r\n",
        "        while True:\r\n",
        "            if current > ts_label:\r\n",
        "                ts_label, label = next(g_label)\r\n",
        "                continue\r\n",
        "            elif current < ts_label:\r\n",
        "                final_labels.append(0)\r\n",
        "                current = next(g_ts)\r\n",
        "                continue\r\n",
        "            final_labels.append(label)\r\n",
        "            current = next(g_ts)\r\n",
        "            ts_label, label = next(g_label)\r\n",
        "    except StopIteration:\r\n",
        "        return np.array(final_labels, dtype=np.int8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-TCML4hLfOe"
      },
      "source": [
        "%%time\r\n",
        "FINAL_LABELS = fill_blank(CHECK_TS, LABELS, np.array(VALIDATION_DF_RAW[TIMESTAMP_FIELD]))\r\n",
        "FINAL_LABELS.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBKtwHKIqYyB"
      },
      "source": [
        "Submit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJOYVsYp6mDW"
      },
      "source": [
        "submission = pd.read_csv('/content/drive/My Drive/dataset/산업보안AI_data/HAI 2.0/sample_submission.csv')\n",
        "submission.index = submission['time']\n",
        "submission.loc[CHECK_TS,'attack'] = LABELS\n",
        "submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnohNeldSs_W"
      },
      "source": [
        "submission.to_csv('/content/drive/My Drive/dataset/산업보안AI_data/HAI 2.0/sample_submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}